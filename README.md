Here's a well-structured **README** description for your GitHub repository:  

---

# NLP with Disaster Tweets   

## Project Overview  
This project aims to classify tweets as **Disaster** or **Not Disaster** using **Natural Language Processing (NLP)** and **Machine Learning (ML)** techniques. It fetches real-time tweets from Twitter using the **Twitter API** and processes them to identify disaster-related content.  

Developed as part of my **internship at Infosys Springboard**, this project helps in disaster response, social media monitoring, and news verification by filtering relevant tweets.  

## 🚀 Features  
Real-time Tweet Fetching – Uses the **Twitter API** to collect live tweets  
Disaster Classification – Machine learning model predicts if a tweet is disaster-related  
User Interface – Built with **Streamlit** for easy interaction  
NLP Processing – Tokenization, stopword removal, and TF-IDF vectorization using **NLTK**  
Model Deployment – Integrated with **Postman** for API testing  

## Tech Stack  
🔹 Python – Core programming language  
🔹 Machine Learning – Random Forest for classification  
🔹 NLTK – NLP preprocessing  
🔹 Twitter API – Fetching real-time tweets  
🔹 Postman – API testing  
🔹 Streamlit – User Interface  


## 📌 How to Run the Project  
1️⃣ Clone this repository:  
   ```bash
   git clone https://github.com/Dhananjay787500/NLP-Disaster-Tweets.git
   cd NLP-Disaster-Tweets
   ```  
2️⃣ Install dependencies:  
   ```bash
   pip install -r requirements.txt
   ```  
3️⃣ Set up Twitter API keys in **.env** file  
4️⃣ Run the Streamlit app:  
   ```bash
   streamlit run app.py
   ```  

## 📜 Acknowledgments  
Special thanks to **Infosys Springboard** for the opportunity to work on this exciting NLP project! 
